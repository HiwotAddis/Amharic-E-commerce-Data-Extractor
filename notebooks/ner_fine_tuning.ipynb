{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27ba6069",
   "metadata": {},
   "source": [
    "# Define Labels and Load CoNLL Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b34b745d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label list: ['B-PRODUCT', 'I-LOC', 'I-PRICE', 'I-PRODUCT', 'O']\n",
      "sentence: [('3pcs', 'B-PRODUCT'), ('silicon', 'I-PRODUCT'), ('brush', 'I-PRODUCT'), ('spatulas', 'I-PRODUCT'), ('እስከ', 'O'), ('260°c', 'O'), ('ሙቀት', 'O'), ('መቆቆም', 'O'), ('የሚችል', 'O'), ('ዋጋ-550ብር', 'I-PRICE'), ('አድራሻ', 'O'), ('ቁ.1', 'O'), ('ስሪ', 'O'), ('ኤም', 'O'), ('ሲቲ', 'O'), ('ሞል', 'O'), ('ሁለተኛ', 'O'), ('ፎቅ', 'O'), ('ቢሮ', 'O'), ('ቁ.', 'O'), ('SL-05A(ከ', 'O'), ('ሊፍቱ', 'O'), ('ፊት', 'O'), ('ለ', 'O'), ('ፊት)', 'O'), ('ቁ.2', 'O'), ('ለቡ', 'I-LOC'), ('መዳህኒዓለም', 'O'), ('ቤተ/ክርስቲያን', 'O'), ('ፊት', 'O'), ('ለፊት', 'O'), ('#ዛም_ሞል', 'O'), ('2ኛ', 'O'), ('ፎቅ', 'O'), ('ቢሮ', 'O'), ('ቁጥር.214', 'O'), ('ለቡ', 'I-LOC'), ('ቅርንጫፍ0973611819', 'O'), ('0909522840', 'O'), ('0923350054', 'O'), ('በTelegram', 'O'), ('ለማዘዝ', 'O'), ('ይጠቀሙ', 'O'), ('@shager_onlinestore', 'O'), ('ለተጨማሪ', 'O'), ('ማብራሪያ', 'O'), ('የቴሌግራም', 'O'), ('ገፃችን', 'O'), ('https://t.me/Shageronlinestore', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# Load and parse CoNLL data\n",
    "def parse_conll(filepath):\n",
    "    sentences, labels = [], []\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        tokens, tags = [], []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if tokens:\n",
    "                    sentences.append(tokens)\n",
    "                    labels.append(tags)\n",
    "                    tokens, tags = [], []\n",
    "            else:\n",
    "                try:\n",
    "                    token, tag = line.split()\n",
    "                    tokens.append(token)\n",
    "                    tags.append(tag)\n",
    "                except ValueError:\n",
    "                    # If any line doesn't contain two elements\n",
    "                    print(f\"Skipping bad line: {line}\")\n",
    "        if tokens:  # Catch last sentence if file doesn't end with newline\n",
    "            sentences.append(tokens)\n",
    "            labels.append(tags)\n",
    "    return sentences, labels\n",
    "\n",
    "# Path to your labeled file\n",
    "conll_file_path = \"../data/labels/conll_labeled_subset.txt\"  # Update if needed\n",
    "\n",
    "# Load data\n",
    "sentences, ner_tags = parse_conll(conll_file_path)\n",
    "\n",
    "# Automatically extract labels\n",
    "all_labels = set(tag for tags in ner_tags for tag in tags)\n",
    "label_list = sorted(all_labels)\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "\n",
    "# Confirm it worked\n",
    "print(\"Label list:\", label_list)\n",
    "print(\"sentence:\", list(zip(sentences[0], ner_tags[0])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b28e94a",
   "metadata": {},
   "source": [
    "# Tokenize & Align Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f90d8021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"xlm-roberta-base\"  # alternatives: \"Davlan/bert-tiny-amharic\", \"Davlan/afro-xlmr-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Tokenize + align labels\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        label_ids = []\n",
    "        prev_word_id = None\n",
    "        for word_id in word_ids:\n",
    "            if word_id is None:\n",
    "                label_ids.append(-100)  # ignored in loss computation\n",
    "            elif word_id != prev_word_id:\n",
    "                label_ids.append(label_to_id[label[word_id]])\n",
    "            else:\n",
    "                label_ids.append(label_to_id[label[word_id]] if label[word_id].startswith(\"I\") else -100)\n",
    "            prev_word_id = word_id\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a126e50",
   "metadata": {},
   "source": [
    "# Prepare Dataset and Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71c38002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bdfac7eb6174afbad312579941788fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2532 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "566ce8066b824307a9781b28a5570ef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/634 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# Create Hugging Face dataset\n",
    "data = Dataset.from_dict({\"tokens\": sentences, \"ner_tags\": ner_tags})\n",
    "\n",
    "# Split into train/validation\n",
    "data = data.train_test_split(test_size=0.2)\n",
    "train_dataset = data[\"train\"]\n",
    "val_dataset = data[\"test\"]\n",
    "\n",
    "# Tokenize\n",
    "train_tokenized = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "val_tokenized = val_dataset.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cd89f7",
   "metadata": {},
   "source": [
    "# Define Model and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d9714c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import TrainingArguments,AutoModelForTokenClassification,Trainer\n",
    "from seqeval.metrics import classification_report\n",
    "\n",
    "# Load model with appropriate number of labels\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n",
    "\n",
    "# Training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./ner_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "# Define metrics\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    preds = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels, true_preds = [], []\n",
    "    for label, pred in zip(labels, preds):\n",
    "        true_label = []\n",
    "        true_pred = []\n",
    "        for l, p in zip(label, pred):\n",
    "            if l != -100:\n",
    "                true_label.append(id_to_label[l])\n",
    "                true_pred.append(id_to_label[p])\n",
    "        true_labels.append(true_label)\n",
    "        true_preds.append(true_pred)\n",
    "    results = metric.compute(predictions=true_preds, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_tokenized,\n",
    "    eval_dataset=val_tokenized,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190c146a",
   "metadata": {},
   "source": [
    "# Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a862eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4075f0",
   "metadata": {},
   "source": [
    "# Evaluate and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba12738b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "metrics = trainer.evaluate()\n",
    "print(metrics)\n",
    "\n",
    "# Save model for future inference\n",
    "trainer.save_model(\"./ner_amharic_model\")\n",
    "tokenizer.save_pretrained(\"./ner_amharic_model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
